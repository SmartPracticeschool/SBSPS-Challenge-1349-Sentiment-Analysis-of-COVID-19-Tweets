# -*- coding: utf-8 -*-
"""Clean Sentiment IBM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1p5V9Fp4KDTJJtZ_gLNKyBrDogD018gFq
"""

!pip install vaderSentiment
!pip install textblob
!pip install wordcloud

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import csv
import re
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')
#import tweepy
from textblob import TextBlob
from wordcloud import WordCloud

data = pd.read_csv(r"Labeled_data.csv")



dataframe = data[['Tweet Content','Tweet Location','sentiment']]
X = pd.DataFrame(dataframe.iloc[:100000,:].values,columns=['Tweets','Location','Sentiment'])
X



#Clean the text

def cleanText(text):
    text = re.sub(r"@[A-Za-z0-9]+","",text)#removing @ mentions
    text = re.sub(r'#','',text)#removing #
    text = re.sub(r'RT[\s]+','',text)# remove RT(retweet)
    text =re.sub(r'https?:\/\/\S+','',text) #removing hyperlinks
    #text =re.sub(r'"',"",text)
    return text

X['Tweets'] = X['Tweets'].apply(cleanText)

X

# Plot the Word Cloud

allWords = ' '.join([twts for twts in X['Tweets']])
wordCloud = WordCloud(width=1000,height=500,random_state=21,max_font_size=250).generate(allWords)
plt.imshow(wordCloud,interpolation='bilinear')
plt.axis('off')
plt.show()

import nltk
nltk.download("stopwords")
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer

"""
corpus=[]# removing punctutations,removing stopwords(a,an,the,and,or),lowercase,
for i in range(0, 100000):
    review = re.sub("[^a-zA-Z]"," ",X['Tweets'][i]) #removing everything except ato z
    review = review.lower()# capital to small
    review = review.split()# i.e. tokenize i.e. preproces for stemming
    ps = PorterStemmer()
    all_stopwords= stopwords.words('english')
    all_stopwords.remove('not')
    review = [ps.stem(word) for word in review if not word in set(all_stopwords)]#remove stopwords and apply stemming
    review = ' '.join(review)
    corpus.append(review)
   
"""

#corpus

#corpus = pd.DataFrame(corpus)
#type(corpus)

"""
import csv

with open('preprocessed.csv', 'w',newline='') as f:
    writer = csv.writer(f)
    writer.writerows(zip(corpus,range(0,100000)))
    
"""

df = pd.DataFrame(pd.read_csv(r"preprocessed.csv"))
final_df = pd.concat([X,df],axis =1)
final_df = final_df.drop(['Tweets','numbering'],axis=1)
final_df.rename(columns={'Tweetss':'Tweets'}, inplace=True)
cols=["Tweets","Location","Sentiment"]
final_df = final_df[cols]
final_df.dropna(inplace=True)
final_df.shape[0]

location=pd.DataFrame(final_df['Location'])
location

corpus_list = list(final_df['Tweets'])
corpus_list

#Creating bag of words model(a sparse matrix)
from sklearn.feature_extraction.text import CountVectorizer

cv = CountVectorizer(max_features=4000)
bag_of_words = cv.fit_transform(corpus_list).toarray()
bag_of_words = pd.DataFrame(bag_of_words)
y = final_df.iloc[:,-1].values

bag_of_words = np.array(bag_of_words)

len(bag_of_words)
len(bag_of_words[0])
bag_of_words.shape
type(bag_of_words)
type(y)
y = np.array(y.reshape(len(y),1))
y=y.astype('int')
y.shape

#spliiting the dataset into training and test
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(bag_of_words,y,test_size=0.2,random_state=0)

"""
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [-1])], remainder='passthrough')
bag_of_words = np.array(ct.fit_transform(bag_of_words))
print(bag_of_words)
"""

from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(random_state = 0)
classifier.fit(X_train, y_train)

type(X_test)

#predicting the test set results
y_pred = classifier.predict(X_test)
#print(np.concatenate(y_pred.reshape(len(y_pred),1)),y_test.reshape(len(y_test),1))

#Making the confusion Matrix
from sklearn.metrics import confusion_matrix,accuracy_score
cm = confusion_matrix(y_test,y_pred)
print(cm)

accuracy_score(y_test,y_pred)

def new_line(new_review):
  
  new_review = re.sub('[^a-zA-Z]', ' ', new_review)
  new_review = new_review.lower()
  new_review = new_review.split()
  ps = PorterStemmer()
  all_stopwords = stopwords.words('english')
  all_stopwords.remove('not')
  new_review = [ps.stem(word) for word in new_review if not word in set(all_stopwords)]
  new_review = ' '.join(new_review)
  new_corpus = [new_review]
  new_X_test = cv.transform(new_corpus).toarray()
  new_y_pred = classifier.predict(new_X_test)
  return int(new_y_pred)

#binal = new_line('I hate this restaurant so much')

#binal

final_df

for region in final_df['Location'].unique():
  pos_name = region + '_tweets_positive' 
  neg_name =  region + '_tweets_negative'
  percentage_pos=0
  percentage_neg=0
  pos_name =  final_df[(final_df['Location']==region) & (final_df['Sentiment'] == 0)]
  neg_name =  final_df[(final_df['Location']==region) & (final_df['Sentiment'] == 1)]
  
  labels = 'Positive', 'Negative'
  sizes = [pos_name.shape[0],neg_name.shape[0]]
  colors = ['yellowgreen', 'lightcoral']
  explode = (0, 0.1)  # explode 1st slice
  plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=90)
  plt.title(region,fontdict = {'verticalalignment':'bottom'})
  plt.show()

positive_per_first =[]
negative_per_first = []
for region in final_df['Location'].unique():
  pos_name = region + '_tweets_positive' 
  neg_name =  region + '_tweets_negative'
  
  pos_name =  final_df[(final_df['Location']==region) & (final_df['Sentiment'] == 0)]
  neg_name =  final_df[(final_df['Location']==region) & (final_df['Sentiment'] == 1)]
  positive_per_first.append(round(pos_name.shape[0]/(pos_name.shape[0]+neg_name.shape[0]),2))
  negative_per_first.append(round(neg_name.shape[0]/(pos_name.shape[0]+neg_name.shape[0]),2))

positive_per_first

lockdown_df = pd.read_csv("lockdown.csv")
lockdown_df

lockdown_df= lockdown_df['text']

array = np.array(lockdown_df)

#Creating bag of words model(a sparse matrix)
from sklearn.feature_extraction.text import CountVectorizer

cv = CountVectorizer(max_features=4000)
predictions = cv.fit_transform(array).toarray()
predictions = pd.DataFrame(bag_of_words)



y_pred = classifier.predict(predictions)

np.reshape(y_pred,y_pred.shape[0],1)

final_array = np.concatenate((array,y_pred),axis=1)

final_pd = pd.DataFrame(final_array)

final_pd







